# Описание проекта

## Релевантность

Есть база знаний с готовыми ответами на вопросы. Пользователь(юрист) отправляет запрос и
получает в ответ текст из одного из файлов, находящегося в базе знаний.
Проблема - если в тексте встречается сразу статья 213 ук рф и 228 ук рф.
Система выдаст в качестве текста сразу и то, и то. Если данные будут различаться, то TF-IDF уже не справляется.

Генерацию лучше не использовать, так как работаем с юридическими данными.

Альтернативное решение:

1. Использование моделей типа BERT для определения контекста(ruBert)
2. Использование контекстуальных эмбеддингов(SBERT, word2vec)
3. Можно глянуть в поле RAG
4. Самый сложный вариант: Использование TF-IDF + контекстуальные эмбеддинги
5. Предложение Хомяка: использование контекстуальных эмбеддингов для сравнение результатов вместо TF-IDF

Таски:

<!-- 1. Увеличить тестовую выборку до 500 вопросов -->

2. Использовать метрики по ссылкам
3. Посмотреть альтернативы косинусного сходства
4. Начать написание. "Разные подходы к поиску" "развитие метода (или модуля) ранжирования вопросно-ответных пар по релевантности"

<!-- 1. Нагенерить тестовые данные и ответы, используя бд хомяка. -->

Чекнуть метрики по ссылкам:

1. <https://www.niklasbuellesbach.com/metrics-that-matter-for-search/>
2. <https://heidloff.net/article/search-evaluations/>
3. <https://streammydata.ru/site-search-metrics-and-how-to-measure-them/>
4. <https://www.linkedin.com/advice/3/youre-trying-evaluate-your-search-engine-performance-omkfc>
5. [link](./chat-Search-Engine-Evaluation-Metrics.md)

   Ссылки:

6. [RAG](https://habr.com/ru/articles/779526/)
7. [RAG 2](https://habr.com/ru/articles/850076/)
8. [sbert](https://developers.sber.ru/portal/products/sbert)
9. [rubert](https://huggingface.co/ai-forever/ruBert-base)
